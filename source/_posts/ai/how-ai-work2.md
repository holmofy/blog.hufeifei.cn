---
title: AI到底是在抄答案还是推理？
date: 2025-09-02
categories: AI
tags: 
- AI
---

你有没有想过AI到底是怎样推理的？这篇文章将带领读者钻进AI的大脑，把它的思考过程一层一层的剥开，让你彻底的搞明白它的那些惊艳的答案到底是通过严密的推理一步一步推导出来的，还是像复读机一样从它以前背过的内容里抄过来的。

这篇文章的内容来自于谷歌DeepMind团队的首席科学家[Denny Zhou](https://dennyzhou.github.io/)在斯坦福大学[CS25 《Transformers United V5》课程](https://web.stanford.edu/class/cs25/)中的讲座。相信通过这篇文章的讲解，你将对AI的理解和使用方式都会发生根本性的改变，你会彻底搞懂为什么AI会一本正经的胡说八道，你会发现其实AI根本没有你想象的那么聪明，但是也没有你想象的那么笨。

首先我们从AI的这个出厂设置说起。其实一个最基础的AI大模型，它的逻辑非常简单，它读过全球几乎所有的书和网站，当你问他的时候，他就会根据读过的资料来连续的预测下一个可能出现的字。就跟很多人说为什么AI不能一次性地把答案输出，非要一个字一个字地去吐，以前还有人跟我说，这是特效，其实不是的。

比如你说“天空是”，它就会从亿万份的资料里面去统计，发现“蓝”这个字跟在后面的概率最高，然后它就会输出“蓝”；接着AI又会把“天空是蓝”作为一个整体重复上面的预测过程，就会发现“色”这个字出现的概率最高，他就继续输出“色”，所以这就有了“天空是蓝色”。

这个机制有个名字叫“**贪婪解码**”。为什么叫“贪婪”，因为它只顾眼前，这就是为什么早期的AI经常是一本正经的胡说八道。比如你问AI一个脑筋急转弯：汤姆的妈妈有三个儿子，老大叫大毛，老二叫二毛，那老三叫什么？它很可能就会回答你三毛。为什么呢？因为“大毛二毛三毛”在它读过的数据里出现的概率太高了，它就直接“贪”了这个最省事的回答，它不会考虑问题里的关键信息就是“汤姆的妈妈”。

那怎么解决这个“贪婪”的问题，那后来科学家就发现了一个点石成金的提示词咒语——一步一步想。这个提示词真的很神奇，为什么我们在技术上不做任何的优化和调整，就只是给AI加上这么一句话，AI的正确率就能大大提升。其实很简单，你加上这句话就相当于强行给他塞了一张草稿纸，并且改变了他要做的任务，那他的任务就不再是直接去预测“汤姆”这个最终的答案，而是变成先预测解题过程中的第一句话，然后在预测第二句话，最后再到解题的结果。虽然表面上还是在预测下一个字，但是实际上他是在预测解题的过程，所以他会从读过的大量的问题里找到相似的解题过程套用过来。比如说他会生成这样的草稿：问题是汤姆妈妈第三个儿子的名字，题目里已经明确提到了汤姆的妈妈，并且提到了两个儿子大毛和二毛，根据人类的语言习惯，汤姆就是他妈妈的儿子，所以第三个儿子就是汤姆。当他把这个完整的逻辑链条以文字的形式一步一步的生成出来之后，汤姆这个正确答案就形成了这个链条末端最高概率的那个词。

所以你看，所谓的AI推理本质上就是在生成最终的答案之前，先生成一串作为草稿的中间步骤，这个草稿打的越长越详细逻辑越严密，那么最终结果就越靠谱。我们并没有给AI装上一个新大脑，只是用一个聪明的指令，逼着他把恐怖的算力用在了打草稿这个正确的事情上面。

那么下一个问题就来了，我们不能每次都在AI的屁股后面提醒他吧。那怎么把它从一个需要去监督的学生，训练成天生就懂推理的大师呢？这就引入了AI推理能力的进化三部曲。

其实这个三部曲很有意思，第一步就是“题海战术”，我们可以雇佣成千上万的人类专家，手写出无数道完美的解题草稿，然后再把这些标准答案一股脑地全部喂给AI，让他去死记硬背，他就像是一个刷题无数的学生，考卷里的所有题型，他都见过，那自然就能考出好成绩，但是如果稍微换一个问法，他马上就抓瞎了，因为他学到的是题，而不是方法，这个学名就叫做“**监督微调**”。

第二个阶段呢就是“**自我博弈**”。那档次就完全不一样了，我们不需要再喂给AI标准答案，而是让AI自己天马行空用各种你想的到想不到的方法去解题，就相当于是让他随便去试。然后这里边就生成了大量的无数的千奇百怪的草稿。然后我们再给他一个判断这个答案是正确还是错误的程序，如果你答对了，我就给这个学生AI的这个解题路径一个大大的奖励。如果你答错了，我就给它一个惩罚，它在经历了几亿万次的自我的试错和奖惩之后，这个学生AI他自己就悟了。他不再是模仿人类，而是真正从这个底层的规律上面就掌握了到底什么才是通往正确答案的最优雅最高效的推理路径。这就是常说的“**强化学习**”。

但是这还不够，一个大师也有偶尔失手的时候，所以第三个阶段叫做“**集体智慧**”，我们可以给AI 99个分身，让他的这个回答增加一点随机性，把同一个问题用不同的思路独立的去算100遍，然后我们把100个答案放在一起公开投票，哪一个答案出现的次数最多，那我们就采信哪一个。这就是用最朴素的统计学原理把这个偶然犯错的可能性降到了最低。这叫“自洽性投票”。

一个顶尖AI的思考能力，其实就是这么一步步进化过来的，从一个只会联想的超级输入法进化成一个被逼着打草稿的普通学生，然后再进化成一个自我博弈的学霸宗师，最终进化成由100个宗师分身组成的学术委员会。

所以回到这个最根本的问题。AI到底是在推理还是在背答案，Denny Zhou就说，我们不要拘泥于这个定义，AI就是在检索中推理，在推理中检索。明白了这一点，我们就可以让AI的效果超越90%的人，那么这里就教大家非常有用的三招。

比如你在解决一个几何题。如果你直接去问，AI有可能会答错，但是如果你在提示词里加上一句话：“回忆相关的问题”，然后再去解决这个问题，那模型他可能会去先回忆检索出来一个关于计算两点之间距离的公式，然后在利用这个知识成功解决当前的面积计算的问题。其实我们在解决复杂问题的时候，可以先去引导模型后退一步，先去思考解决这一类问题所需要的一般性的原则，或者抽象的概念，然后再把这些原则用在这个具体的问题里面。

第二招是“步骤分解法”，你要像一个项目经理一样，主动的把你的任务给拆解掉，你主动帮他搭好一个思考的架子，它就能够通过检索的这种方式在你的这个框架里面填出一个最完美的内容。比如你不要去说给我做一个云南的旅游攻略，而是要说我们来规划一个为期7天的云南家庭游。第一步请列出适合家庭游的三个核心城市及理由，第二步为我们规划每一天的详细行程，包括景点和交通，第三步估算一下人均的预算。这个方法其实特别适合于没有深度思考能力的模型。

第三招是“自我审查法”，我们没办法直接用AI的百人投票，但是我们可以让AI自己检查自己，你得到一个答案之后，你不要先着急采纳，你可以去追问AI说，你确定吗，你重新审视一下你的答案，找出其中可能存在的3个逻辑漏洞。这招相当于是让AI自己跟自己打架，他立刻就会启动更深度的思考，去检查和修正之前的一些答案，准确率会再上一个台阶。

所以看懂了这些你就会发现AI虽然不是一个有感情的人，但是他可以被精确的引导，被严格的塑造，它是一个究极的逻辑机器。所以未来真正能够拉开人与人差距的，不是你掌握了多少知识，而是你能不能设计出一个高明的问题，一个明妙的指令，去帮助这个地球上最强的外挂，帮你一步一步的想出颠覆整个牌桌的答案。





---
title: 信息检索的三种方式：稀疏检索、稠密检索、知识图谱增强检索
date: 2025-7-2
categories: 数据结构与算法
mathjax: true
tags: 
- BM25
- Embedding
- GraphRAG
keywords:
- BM25
- Embedding
- GraphRAG
---

## 核心概念概述

1.  **检索（Retrieval）**：在信息检索中，指的是从一个大规模文档集合中找到与用户查询相关的文档子集的过程。这是搜索引擎、问答系统等的核心步骤。
2.  **表示（Representation）**：如何用计算机可处理的形式来表示查询和文档，是检索效果好坏的关键。

---

## 1. 稀疏检索

![tf-idf](https://miro.medium.com/v2/resize:fit:1400/1*lGwuTdBuX3Jnzz4Mi9gSTA.png)

*   **核心思想**：
    *   使用高维、稀疏的向量来表示查询和文档。
    *   向量的维度通常对应于词汇表中的所有词项（Term）。
    *   每个维度的值表示该词项在查询或文档中的重要性（权重）。
    *   权重计算通常基于词频（TF）、逆文档频率（IDF）等统计信息。
    *   **关键特征：向量的大部分维度值为 0（稀疏），只有文档或查询中实际出现的词对应的维度值不为 0。**
*   **代表算法**：
    *   **TF-IDF**：词频-逆文档频率，经典权重计算方法。
    *   **BM25**：TF-IDF 的改进和概率化版本，**是目前最主流的稀疏检索算法**，效果稳定且优异。它考虑了词频、文档长度、逆文档频率等因素。
    以下是 **TF-IDF** 和 **BM25** 的完整计算公式，使用 LaTeX 格式呈现：
    1. **TF-IDF 公式**
    TF-IDF 由两部分组成：**词频 (TF)** 和 **逆文档频率 (IDF)**。

       * 词频(TF) $\mathrm{tf}(t, d) = \frac{f_{t,d}}{\sum_{t' \in d} f_{t',d}}$
       其中：
       - $\(f_{t,d}\)$：词项 $\(t\)$ 在文档 $\(d\)$ 中的出现次数
       - $\(\sum_{t' \in d} f_{t',d}\)$：文档 $\(d\)$ 的总词项数
   
       * 逆文档频率 (IDF) $\mathrm{idf}(t, D) = \log \frac{N}{n_t}$
         其中：
         - $\(N\)$：语料库中文档总数
         - $\(n_t\)$：包含词项 $\(t\)$ 的文档数量
   
       * TF-IDF 最终分数
         $\mathrm{tfidf}(t, d, D) = \mathrm{tf}(t, d) \times \mathrm{idf}(t, D)$


    2. **BM25 公式**
    BM25 在 TF-IDF 基础上引入了**词频饱和控制**和**文档长度归一化**。
    完整 BM25 分数
    $\mathrm{BM25}(d, q) = \sum_{t \in q} \mathrm{IDF}(t) \cdot \frac{f_{t,d} \cdot (k_1 + 1)}{f_{t,d} + k_1 \cdot \left(1 - b + b \cdot \frac{|d|}{\mathrm{avgdl}}\right)}$
       组件详解：
      * **逆文档频率 (IDF)**  
         $\mathrm{IDF}(t) = \log \left( \frac{N - n_t + 0.5}{n_t + 0.5} \right)$
         - $\(N\)$：文档总数  
         - $\(n_t\)$：包含词项 $\(t\)$ 的文档数量  
         - $\(+0.5\)$：平滑因子，避免除零错误
      
      * **词频饱和控制**  
         $\frac{f_{t,d} \cdot (k_1 + 1)}{f_{t,d} + k_1}$
         - $\(f_{t,d}\)$：词项 $\(t\)$ 在文档 $\(d\)$ 中的频率  
         - $\(k_1\)$：饱和度参数（默认值 $\(k_1 \in [1.2, 2.0]\)$）  
         - 作用：抑制高频词的过度影响（非线性增长）
      
      * **文档长度归一化**  
         $1 - b + b \cdot \frac{|d|}{\mathrm{avgdl}}$
         - $\(|d|\)$：当前文档长度（词项数量）  
         - $\(\mathrm{avgdl}\)$：语料库中文档平均长度  
         - $\(b\)$：长度归一化参数（默认 $\(b = 0.75\)$）  
         - 作用：惩罚长文档，避免长度偏差  
   
      3. 关键改进对比

      | 组件             | TF-IDF                          | BM25 改进                                                                 |
      |------------------|---------------------------------|--------------------------------------------------------------------------|
      | **词频 (TF)**    | 线性增长： $\(f_{t,d}\)$           | 非线性饱和： $\(\frac{f_{t,d} \cdot (k_1 + 1)}{f_{t,d} + k_1}\)$             |
      | **文档长度处理** | 无                              | 显式归一化： $\(1 - b + b \cdot \frac{\|d\|}{\mathrm{avgdl}}\)$                 |
      | **IDF 平滑**     | 基础： $\(\log \frac{N}{n_t}\)$    | 鲁棒平滑： $\(\log \frac{N - n_t + 0.5}{n_t + 0.5}\)$                        |
      | **可调参数**     | 无                              | 参数化： $\(k_1\)$ (控制饱和度) 和 $\(b\)$ (控制长度惩罚)                       |
   
      4. 核心优势总结
      BM25 通过 **非线性 TF 饱和** 和 **文档长度归一化** 解决了 TF-IDF 的两大缺陷：  
        + **避免长文档主导结果**（例如一篇 10,000 词的文档偶然多次出现查询词）  
        + **抑制关键词重复堆砌**（例如关键词重复 100 次不代表相关性线性增加 100 倍）  
        + **参数可调**（通过 $\(k_1\)$ 和 $\(b\)$ 适配不同场景的数据分布）
         ![tf score](https://ethen8181.github.io/machine-learning/search/img/tf_comparison.png)
*   **计算相关性**：
    *   查询向量 `q` 和文档向量 `d` 的相关性得分通常通过它们向量的点积（Dot Product）`q · d` 计算。
    *   点积的结果本质上是查询和文档中共同出现的词项权重乘积之和。
*   **优点**：
    *   **可解释性强**：得分直接来源于共同词项及其权重，容易理解为什么某文档被召回。
    *   **效率高**：可以利用倒排索引（Inverted Index）实现非常高效的检索（仅需处理包含查询词的文档）。索引构建和查询速度通常很快。
    *   **无需训练数据**：经典方法（如BM25）不需要特定的训练语料来学习参数（虽然参数可以调优）。
    *   **对精确匹配效果好**：当查询包含明确关键词时，效果通常很好。
*   **缺点**：
    *   **词汇鸿沟问题**：无法处理同义词（`car` vs `automobile`）、近义词（`big` vs `large`）、一词多义（`bank` 可以是河岸或银行）等问题。查询和文档必须包含相同的词项才能匹配。
    *   **语义理解有限**：主要基于词形（lexical）匹配，缺乏对深层语义的理解。
    *   **长尾/复杂查询效果可能不佳**：对于表述模糊、依赖语义而非关键词的查询效果可能不如稠密检索。
*   **典型应用**：
    *   传统搜索引擎（如早期的Google，现在核心BM25仍是重要组成部分）。
    *   需要高效、可解释检索结果的场景。
    *   作为更复杂检索系统的第一级召回器（快速筛选候选文档）。

---

## 2. 稠密检索

![Embedding](https://cosminsanda.com/assets/images/pgvector-seaorm/embedded-intuition.png)

*   **核心思想**：
    *   使用低维、稠密的向量（通常称为嵌入向量）来表示查询和文档。
    *   向量维度通常在几百到几千维。
    *   **关键特征：向量的每个维度通常都有非零值（稠密），每个维度代表某种潜在的语义特征。这些特征是通过深度神经网络学习得到的。**
*   **代表模型**：
    *   **双塔模型（Dual Encoder）**：最常见架构。一个神经网络编码器将查询映射为向量 `q`，另一个（通常共享参数）将文档映射为向量 `d`。
    *   **预训练语言模型驱动**：如基于BERT的模型（DPR, ANCE, ColBERT等）。这些模型利用大规模语料上预训练得到的强大语义理解能力。
*   **计算相关性**：
    *   查询向量 `q` 和文档向量 `d` 的相关性得分通过计算它们之间的向量相似度得到，最常用的是余弦相似度（Cosine Similarity）`cos(q, d)`。
    *   相似度衡量的是它们在低维语义空间中的“距离”或“方向一致性”。
*   **优点**：
    *   **强大的语义表示能力**：能有效捕捉同义词、近义词、语义关联（`AI` 和 `机器学习`），缓解词汇鸿沟问题。
    *   **对复杂/模糊查询效果好**：能更好地理解查询的意图和上下文语义。
    *   **迁移学习潜力**：预训练语言模型的知识可以迁移到特定领域任务。
*   **缺点**：
    *   **可解释性差**：难以解释为什么某个向量代表了某个语义，以及为什么两个向量相似。是一个“黑盒”。
    *   **需要大量训练数据**：训练高质量的稠密检索模型通常需要大量的标注数据（查询-相关文档对）。
    *   **计算开销大**：
        *   **训练成本高**：需要GPU资源和时间训练模型。
        *   **索引构建复杂**：需要为所有文档生成稠密向量（嵌入），存储空间需求大（比稀疏索引大得多）。
        *   **检索效率挑战**：计算所有文档向量的余弦相似度在大规模库中非常慢（`O(N)`）。通常需要借助**近似最近邻搜索**技术来加速。
    *   **对精确匹配可能不如BM25**：如果查询就是精确的关键词组合，BM25可能更直接有效。
*   **典型应用**：
    *   开放域问答（Open-Domain QA）。
    *   需要深度语义理解的检索任务（如对话系统、推荐系统）。
    *   作为稀疏检索的补充，提高召回结果的相关性和多样性（混合检索）。

---

## 3. 知识图谱增强检索

![RAG vs. GraphRAG](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b4ee6fd-fcc2-4e02-9f6c-472615c77730_800x830.gif)

*   **核心思想**：
    *   **不是独立的检索模型，而是利用外部结构化的知识图谱来增强现有的检索模型（稀疏或稠密）。**
    *   知识图谱是一个由实体（Entity）、实体属性（Attribute）和实体间关系（Relation）组成的语义网络。
    *   目标是将KG中包含的丰富语义信息（实体链接、关系、类型、描述等）融入到检索过程中，提升对查询和文档的理解。
*   **增强方式**：
    1.  **查询扩展/重写**：
        *   识别查询中的实体。
        *   利用KG查找该实体的别名、同义词、相关实体（如：`Steve Jobs` -> `Apple Inc.` 创始人 -> `Apple` 产品 `iPhone`）。
        *   将扩展得到的信息（别名、相关实体词）加入到原始查询中，再用稀疏/稠密模型检索。
    2.  **文档表示增强**：
        *   识别文档中的实体。
        *   将实体在KG中的信息（类型、关系、描述文本的嵌入）融入到文档的向量表示中（稀疏或稠密）。
    3.  **联合检索/排序**：
        *   在检索或排序阶段，不仅考虑查询-文档的匹配度，也考虑文档中实体与查询中实体的语义关联度（通过KG中的路径、关系等计算）。
        *   构建图神经网络模型，同时建模文本信息和KG结构信息。
    4.  **实体链接（Entity Linking）**：将文本中提到的实体指称项链接到KG中对应的实体，这是很多增强方法的基础步骤。
*   **优点**：
    *   **引入结构化知识**：提供额外的、明确的语义信息，弥补纯文本理解的不足。
    *   **提升语义理解深度**：更好地理解实体间关系、实体类型、背景知识。
    *   **缓解歧义**：帮助区分一词多义（利用实体上下文）。
    *   **可解释性潜力**：通过实体链接和关系路径，可以提供部分解释（如：文档被召回是因为提到了与查询实体相关的另一个实体X）。
*   **缺点**：
    *   **依赖高质量KG**：检索效果高度依赖于所使用的知识图谱的覆盖面、准确度和时效性。构建和维护高质量KG成本高昂。
    *   **实体链接错误传播**：如果实体链接步骤出错，后续增强可能引入噪声甚至错误。
    *   **系统复杂度增加**：需要集成实体识别、链接、KG查询等多个模块，系统架构更复杂。
    *   **领域依赖性强**：特定领域的KG可能难以获取或构建。
*   **典型应用**：
    *   需要深度领域知识的检索（如医疗、金融、法律）。
    *   百科问答、事实核查。
    *   需要理解实体间复杂关系的场景。
    *   任何希望利用结构化知识提升纯文本检索效果的领域。

---

## 总结对比表

| 特性               | 稀疏检索 (如 BM25)             | 稠密检索 (如 DPR, ANCE)             | 知识图谱增强检索 (KG-Augmented)            |
| :----------------- | :----------------------------- | :--------------------------------- | :--------------------------------------- |
| **核心表示**       | 高维稀疏向量 (词袋)            | 低维稠密向量 (嵌入)                 | **利用KG增强**稀疏或稠密向量/检索过程     |
| **匹配基础**       | 词形 (Lexical) 匹配            | 语义 (Semantic) 匹配                | 语义 + 结构化知识                        |
| **解决词汇鸿沟**   | 弱                             | 强                                 | 强 (通过实体、同义词、关系)              |
| **可解释性**       | **高** (基于词项)              | **低** (黑盒向量)                  | 中等 (依赖实体链接和关系路径)            |
| **训练数据需求**   | 无/低 (BM25参数可调)           | **高** (需要大量标注对)            | 中等/高 (依赖KG和可能的链接模型训练)     |
| **索引/检索效率**  | **非常高** (倒排索引)          | 较低 (需ANN加速稠密向量搜索)        | 中等 (取决于基础检索模型+KG操作开销)     |
| **计算开销(训练)** | 极低                           | **非常高** (GPU训练)               | 高 (KG构建/维护 + 模型训练)              |
| **计算开销(推理)** | 低                             | 中等/高 (向量计算/ANN)              | 中等 (基础模型推理 + KG查询/计算)        |
| **典型优势场景**   | 关键词明确查询，高效召回       | 语义复杂/模糊查询，理解深层含义     | 需要领域知识、理解实体关系的查询         |
| **主要缺点**       | 词汇鸿沟，语义理解弱           | 可解释性差，需训练数据，效率挑战     | 依赖KG质量，系统复杂，实体链接可能出错   |

## 发展趋势与融合

*   **混合检索**：当前最先进的检索系统通常**结合稀疏检索和稠密检索**。例如：
    *   第一级：用BM25快速召回一批候选文档（高召回）。
    *   第二级：用稠密检索模型对候选文档进行精细化重排（高精度）。
    *   或者，将稀疏检索分数和稠密检索分数线性融合（如 `score = α * BM25 + β * Dense`）。
*   **知识融入稠密模型**：研究者致力于将KG知识直接编码到预训练语言模型中（如在预训练或微调阶段融入KG信息），让模型本身学习到更多结构化知识，减少对外部KG查询的依赖。
*   **端到端学习**：探索将KG信息、检索、排序甚至答案生成整合到一个端到端的可训练框架中。
